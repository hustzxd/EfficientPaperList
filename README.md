# EfficientPaper
Pruning, Quantization and efficient-inference/training paper list.

https://github.com/he-y/Awesome-Pruning

https://github.com/htqin/awesome-model-quantization

https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression/tree/master

https://github.com/AojunZhou/Efficient-Deep-Learning

https://github.com/chester256/Model-Compression-Papers

1. Add meta information in ./meta
2. Run `python scripts/generate_paper_list.py`


|    | meta                           | title                                                                                                                     | publication   | code                                               | note                       |
|---:|:-------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:--------------|:---------------------------------------------------|:---------------------------|
|  0 | [m](./meta/templat2.prototxt)  | (abbr) paper title                                                                                                        | ICLR-2022     |                                                    |                            |
|  1 | [m](./meta/sparsegpt.prototxt) | [ (SparseGPT) SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot.](https://arxiv.org/abs/2301.00774) | arXiv-2023    | [Pytorch](https://github.com/IST-DASLab/sparsegpt) | [note](notes/SparseGPT.md) |