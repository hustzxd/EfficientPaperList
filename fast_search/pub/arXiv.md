|    | meta                                       | title                                                                                                            | publication   |   year | code                                               | note                                       | cover                                                                |
|---:|:-------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|:--------------|-------:|:---------------------------------------------------|:-------------------------------------------|:---------------------------------------------------------------------|
|  0 | [RPTQ](../../meta/RPTQ.prototxt)           | [RPTQ: Reorder-based Post-training Quantization for Large Language Models](https://arxiv.org/pdf/2304.01089.pdf) | arXiv         |   2023 | [PyTorch](https://github.com/hahnyuan/RPTQ4LLM)    |                                            |                                                                      |
|  1 | [SparseGPT](../../meta/sparsegpt.prototxt) | [SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot.](https://arxiv.org/pdf/2301.00774.pdf) | arXiv         |   2023 | [Pytorch](https://github.com/IST-DASLab/sparsegpt) | [note](../../notes/sparsegpt/SparseGPT.md) | <img width='400' alt='image' src='../../notes/sparsegpt/cover.jpg)'> |