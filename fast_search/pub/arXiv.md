|    | meta                           | title                                                                                                                         | publication   | code                                               | note                       |
|---:|:-------------------------------|:------------------------------------------------------------------------------------------------------------------------------|:--------------|:---------------------------------------------------|:---------------------------|
|  0 | [m](./meta/RPTQ.prototxt)      | [ (RPTQ) RPTQ: Reorder-based Post-training Quantization for Large Language Models](https://arxiv.org/pdf/2304.01089.pdf)      | arXiv-2023    | [PyTorch](https://github.com/hahnyuan/RPTQ4LLM)    |                            |
|  1 | [m](./meta/sparsegpt.prototxt) | [ (SparseGPT) SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot.](https://arxiv.org/pdf/2301.00774.pdf) | arXiv-2023    | [Pytorch](https://github.com/IST-DASLab/sparsegpt) | [note](notes/SparseGPT.md) |